/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/utils/hub.py:125: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-05-30 17:21:56,323][datasets][INFO] - PyTorch version 2.3.0 available.
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/modeling_utils.py:4487: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/guidance/chat.py:73: UserWarning: Chat template {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>

'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>

' }} was unable to be loaded directly into guidance.
                        Defaulting to the ChatML format which may not be optimal for the selected model. 
                        For best results, create and pass in a `guidance.ChatTemplate` subclass for your model.
  warnings.warn(f"""Chat template {chat_template} was unable to be loaded directly into guidance.
oracle.evaluate
evaluation: {'analysis': 'Response A is better than Response B.', 'answer': 'Equal'}
time_taken: 22.419424772262573
oracle.is_quality_preserved
quality_eval: {'original_analysis': 'Response A is better than Response B.', 'original_answer': 'Equal', 'followup_analysis': "Both responses provide a thorough analysis of the role of symbolism in 'To Kill a Mockingbird' and its impact on understanding the novel's themes. However, Response A stands out for its clarity, coherence, and attention to detail.", 'followup_answer': 'Equal', 'quality_preserved': True}
time_taken: 42.34528613090515
oracle.test
test_eval: {'original_analysis': 'Response A is better than Response B.', 'original_answer': 'Equal', 'followup_analysis': "Both responses provide a thorough analysis of the role of symbolism in 'To Kill a Mockingbird' and its impact on understanding the novel's themes. However, Response A stands out for its clarity, coherence, and attention to detail.", 'followup_answer': 'Equal', 'original_label': 1, 'followup_label': 5, 'original_pred': 3, 'followup_pred': 3, 'pred_correct': 0}
time_taken: 47.33846092224121
[2024-05-30 17:24:08,260][__main__][INFO] - Loading model: TechxGenus/Meta-Llama-3-70B-Instruct-GPTQ from /data2/.shared_models...
[2024-05-30 17:24:14,337][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/utils/modeling.py:1365: UserWarning: Current model requires 32930743552 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.
  warnings.warn(
Error executing job with overrides: []
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/oracles/relative3.py", line 190, in test
    oracle = RelativeOracle3(cfg.oracle_args)
  File "/local1/borito1907/impossibility-watermark/oracles/relative3.py", line 22, in __init__
    self.llm = models.Transformers(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/guidance/models/transformers/_transformers.py", line 283, in __init__
    TransformersEngine(model, tokenizer, compute_log_probs, chat_template=chat_template, **kwargs), echo=echo
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/guidance/models/transformers/_transformers.py", line 151, in __init__
    self.model_obj = self._model(model, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/guidance/models/transformers/_transformers.py", line 183, in _model
    model = transformers.AutoModelForCausalLM.from_pretrained(model, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3826, in from_pretrained
    dispatch_model(model, **device_map_kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/big_modeling.py", line 419, in dispatch_model
    attach_align_device_hook_on_blocks(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 648, in attach_align_device_hook_on_blocks
    attach_align_device_hook_on_blocks(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 648, in attach_align_device_hook_on_blocks
    attach_align_device_hook_on_blocks(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 648, in attach_align_device_hook_on_blocks
    attach_align_device_hook_on_blocks(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 611, in attach_align_device_hook_on_blocks
    attach_align_device_hook(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 504, in attach_align_device_hook
    attach_align_device_hook(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 504, in attach_align_device_hook
    attach_align_device_hook(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 495, in attach_align_device_hook
    add_hook_to_module(module, hook, append=True)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 157, in add_hook_to_module
    module = hook.init_hook(module)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/hooks.py", line 304, in init_hook
    set_module_tensor_to_device(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 392, in set_module_tensor_to_device
    new_value = old_value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
