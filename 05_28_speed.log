/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
CUDA extension not installed.
CUDA extension not installed.
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/runpy.py:126: RuntimeWarning: 'mutators.sentence' found in sys.modules after import of package 'mutators', but prior to execution of 'mutators.sentence'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
CUDA_VISIBLE_DEVICES: 1,2
WORLD_SIZE: 2
{'type': 'sentence', 'use_system_profile': True, 'system_profile': 'You are a copy editor tasked to enforce text quality.', 'num_retries': 5, 'model_name_or_path': 'MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ', 'model_cache_dir': '${model_cache_dir}', 'revision': 'main', 'device_map': '${generator_args.device_map}', 'trust_remote_code': '${generator_args.trust_remote_code}', 'max_new_tokens': '${generator_args.max_new_tokens}', 'do_sample': '${generator_args.do_sample}', 'temperature': '${generator_args.temperature}', 'top_p': '${generator_args.top_p}', 'top_k': '${generator_args.top_k}', 'repetition_penalty': '${generator_args.repetition_penalty}', 'use_pydantic_parser': True}
[2024-05-28 12:42:08,283][__main__][INFO] - Type of pipeline was: <class 'NoneType'>
[2024-05-28 12:42:08,283][__main__][INFO] - Initializing a new Text Mutator pipeline from cfg...
[2024-05-28 12:42:08,283][model_builders.pipeline][INFO] - Initializing MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.
[2024-05-28 12:42:08,397][auto_gptq.modeling._base][WARNING] - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.
WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:
1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.
2. You are using pytorch without CUDA support.
3. CUDA and nvcc are not installed in your device.
[2024-05-28 12:42:08,397][auto_gptq.modeling._base][WARNING] - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:
1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.
2. You are using pytorch without CUDA support.
3. CUDA and nvcc are not installed in your device.
INFO - The layer lm_head is not quantized.
[2024-05-28 12:42:09,108][auto_gptq.modeling._base][INFO] - The layer lm_head is not quantized.
[2024-05-28 12:42:12,004][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Error executing job with overrides: []
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/mutators/sentence.py", line 153, in test
    text_mutator = SentenceMutator(cfg.mutator_args)
  File "/local1/borito1907/impossibility-watermark/mutators/sentence.py", line 50, in __init__
    self.pipeline = PipeLineBuilder(cfg).pipeline
  File "/local1/borito1907/impossibility-watermark/model_builders/pipeline.py", line 28, in __init__
    self._init_model(self.cfg)
  File "/local1/borito1907/impossibility-watermark/model_builders/pipeline.py", line 53, in _init_model
    self.model = AutoGPTQForCausalLM.from_quantized(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/auto_gptq/modeling/auto.py", line 135, in from_quantized
    return quant_func(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/auto_gptq/modeling/_base.py", line 1209, in from_quantized
    raise ValueError(
ValueError: The model MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ can not be converted to use the Marlin kernel for the following reason: AutoGPTQ is not compiled with the Marlin kernel, with the following error: /local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/autogptq_marlin_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi, which is not supported by Marlin kernel.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
