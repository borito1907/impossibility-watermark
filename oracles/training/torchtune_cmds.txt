# Reproduction Steps

0. Install from source: https://pytorch.org/torchtune/main/install.html#install-via-git-clone
1. Modify torchtune two source files to work with custom datasets. 
 - Location 1: torchtune/datasets/_chat.py
 - Required Change: Add custom dataset with hardcoded values
 - Code 1:

# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple

import ast
import json
import numpy as np

from datasets import load_dataset
from torch.utils.data import Dataset
from torchtune.config._utils import _get_chat_format
from torchtune.data import (
    ChatFormat,
    CROSS_ENTROPY_IGNORE_IDX,
    Message,
    openai_to_llama2_messages,
    sharegpt_to_llama2_messages,
    validate_messages,
)
from torchtune.modules.tokenizers import Tokenizer


class ChatDataset(Dataset):
    """
    Class that supports any custom dataset with multiturn conversations.

    The general flow from loading a sample to tokenized prompt is:
    load sample -> apply transform -> foreach turn{format into template -> tokenize}

    If the column/key names differ from the expected names in the ``ChatFormat``,
    then the ``column_map`` argument can be used to provide this mapping.

    Use ``convert_to_messages`` to prepare your dataset into the Llama2 chat format
    and roles::

        [
            Message(
                role=<system|user|assistant>,
                content=<message>,
            ),
            ...
        ]

    This class supports multi-turn conversations. If a tokenizer sample with multiple
    turns does not fit within ``max_seq_len`` then it is truncated.

    Args:
        tokenizer (Tokenizer): Tokenizer used to encode data. Tokenize must implement an ``encode`` and ``decode`` method.
        source (str): path string of dataset, anything supported by Hugging Face's ``load_dataset``
            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)
        convert_to_messages (Callable[[Mapping[str, Any]], List[Message]]): function that keys into the desired field in the sample
            and converts to a list of :class:`~torchtune.data.Message` that follows the Llama format with the expected keys
        chat_format (Optional[ChatFormat]): template used to format the chat. This is used to add structured text around the actual
            messages, such as the [INST] tags in Llama2 and in Mistral. The extra text will still get tokenized as normal text, not
            as special tokens. In models like Llama3 where the tokenizer adds tags as special tokens, ``chat_format`` is not needed,
            unless you want to structure messages in a particular way for inference. If the placeholder variable names in the
            template do not match the column/key names in the dataset, use ``column_map`` to map them. For a list of all possible
            chat formats, check out :ref:`chat_formats`. Default: None.
        max_seq_len (int): Maximum number of tokens in the returned input and label token id lists.
        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
    """

    def __init__(
        self,
        *,
        tokenizer: Tokenizer,
        source: str,
        convert_to_messages: Callable[[Mapping[str, Any]], List[Message]],
        chat_format: Optional[ChatFormat] = None,
        max_seq_len: int,
        train_on_input: bool = False,
        **load_dataset_kwargs: Dict[str, Any],
    ) -> None:
        self._tokenizer = tokenizer
        self._data = load_dataset(source, **load_dataset_kwargs)
        self._convert_to_messages = convert_to_messages
        self.chat_format = chat_format
        self.max_seq_len = max_seq_len
        self.train_on_input = train_on_input

    def __len__(self):
        return len(self._data)

    def __getitem__(self, index: int) -> Tuple[List[int], List[int]]:
        sample = self._data[index]
        return self._prepare_sample(sample)

    def _prepare_sample(self, sample: Mapping[str, Any]) -> Tuple[List[int], List[int]]:
        messages = self._convert_to_messages(sample)
        if self.chat_format is not None:
            messages = self.chat_format.format(messages)
        validate_messages(messages)
        tokens, mask = self._tokenizer.tokenize_messages(
            messages, max_seq_len=self.max_seq_len
        )
        # Wherever mask == True, set to CROSS_ENTROPY_IGNORE_IDX. Otherwise keep as tokens
        labels = list(np.where(mask, CROSS_ENTROPY_IGNORE_IDX, tokens))
        assert len(tokens) == len(labels)

        return tokens, labels


def chat_dataset(
    *,
    tokenizer: Tokenizer,
    source: str,
    conversation_style: str,
    chat_format: Optional[str] = None,
    max_seq_len: int,
    train_on_input: bool = False,
    **load_dataset_kwargs: Dict[str, Any],
) -> ChatDataset:
    """
    Build a configurable dataset with conversations. This method should be
    used to configure a custom chat dataset from the yaml config instead of
    using :class:`~torchtune.datasets.ChatDataset` directly, as it is made to be config friendly.

    Args:
        tokenizer (Tokenizer): Tokenizer used to encode data. Tokenize must implement an ``encode`` and ``decode`` method.
        source (str): path string of dataset, anything supported by Hugging Face's ``load_dataset``
            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)
        conversation_style (str): string specifying expected style of conversations in the dataset
            for automatic conversion to the Llama style. Supported styles are: "sharegpt"
        chat_format (Optional[str]): name of ``ChatFormat`` class used to format the messages. See the description in
            :class:`~torchtune.datasets.ChatDataset` for more details. For a list of all possible chat formats,
            check out :ref:`chat_formats`. Default: None.
        max_seq_len (int): Maximum number of tokens in the returned input and label token id lists.
        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to `load_dataset`.

    Examples:
        >>> from torchtune.datasets import chat_dataset
        >>> dataset = chat_dataset(
        ...   tokenizer=tokenizer,
        ...   source="HuggingFaceH4/no_robots",
        ...   conversation_style="sharegpt",
        ...   chat_format=ChatMLFormat,
        ...   max_seq_len=2096,
        ...   train_on_input=True
        ... )

    This can also be accomplished via the yaml config::

        dataset:
            _component_: torchtune.datasets.chat_dataset
            source: HuggingFaceH4/no_robots
            conversation_style: sharegpt
            chat_format: ChatMLFormat
            max_seq_len: 2096
            train_on_input: True

    Returns:
        ChatDataset: the configured :class:`~torchtune.datasets.ChatDataset`

    Raises:
        ValueError: if the conversation format is not supported
    """
    if conversation_style == "sharegpt":
        convert_to_messages = sharegpt_to_llama2_messages
    elif conversation_style == "openai":
        convert_to_messages = openai_to_llama2_messages
    else:
        raise ValueError(f"Unsupported conversation style: {conversation_style}")

    return ChatDataset(
        tokenizer=tokenizer,
        source=source,
        convert_to_messages=convert_to_messages,
        chat_format=_get_chat_format(chat_format) if chat_format is not None else None,
        max_seq_len=max_seq_len,
        train_on_input=train_on_input,
        **load_dataset_kwargs,
    )

def format_answer(label):
    answers = {
        "heroinUse": bool(label[0]),
        "cocaineUse": bool(label[1]),
        "methamphetamineUse": bool(label[2]),
        "benzodiazepineUse": bool(label[3]),
        "prescriptionOpioidsUse": bool(label[4]),
        "marijuanaUse": bool(label[5]),
        "fentanylUse": bool(label[6]),
        "injectionDrugUse": bool(label[7]),
        "drugUse": bool(label[8])
    }

    return json.dumps(answers, indent=4)

def message_converter(sample: Mapping[str, Any], train_on_input=None) -> List[Message]:
    input_msg = sample["text"]
    output_msg = format_answer(np.array(ast.literal_eval(sample["label"])))

    user_message = Message(
        role="user",
        content=input_msg,
        masked=False,  # True if not training on prompt
    )
    assistant_message = Message(
        role="assistant",
        content=output_msg,
        masked=False,
    )
    # A single turn conversation
    messages = [user_message, assistant_message]

    return messages

def custom_dataset(
    *,
    tokenizer: Tokenizer,
    max_seq_len: int = 2048,  # You can expose this if you want to experiment
) -> ChatDataset:

    return ChatDataset(
        tokenizer=tokenizer,
        # For local csv files, we specify "csv" as the source, just like in
        # load_dataset
        source="csv",
        convert_to_messages=message_converter,
        # Llama3 does not need a chat format
        chat_format=None,
        max_seq_len=max_seq_len,
        # To load a local file we specify it as data_files just like in
        # load_dataset
        data_files="/data2/fabricehc/overdose/data/processed/under_over_sample/train.csv",
        split="train"
    )

 - Location 2: torchtune/datasets/__init__.py
 Required Change: Adding custom_dataset to init for imports
 - Code 2: 

# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

from torchtune.datasets._alpaca import alpaca_cleaned_dataset, alpaca_dataset
from torchtune.datasets._chat import chat_dataset, ChatDataset, custom_dataset
from torchtune.datasets._grammar import grammar_dataset
from torchtune.datasets._instruct import instruct_dataset, InstructDataset
from torchtune.datasets._samsum import samsum_dataset
from torchtune.datasets._slimorca import slimorca_dataset
from torchtune.datasets._stack_exchanged_paired import stack_exchanged_paired_dataset

__all__ = [
    "alpaca_dataset",
    "alpaca_cleaned_dataset",
    "grammar_dataset",
    "samsum_dataset",
    "stack_exchanged_paired_dataset",
    "InstructDataset",
    "slimorca_dataset",
    "ChatDataset",
    "instruct_dataset",
    "chat_dataset",
    "custom_dataset"
]

- Location 3: torchtune/utils/quantization.py
- Required Change: Remove Int8DynActInt4WeightQuantizer, not found in torchao==0.1
- Code 3:

# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

from typing import Any, Callable, Optional

import torch
from torchao.quantization.quant_api import (
    apply_weight_only_int8_quant,
    Int4WeightOnlyGPTQQuantizer,
    Int4WeightOnlyQuantizer,
    # Int8DynActInt4WeightQuantizer,
    Quantizer,
)

__all__ = [
    "Int4WeightOnlyQuantizer",
    "Int4WeightOnlyGPTQQuantizer",
    "Int8WeightOnlyQuantizer",
    # "Int8DynActInt4WeightQuantizer",
    "get_quantizer_mode",
]


class Int8WeightOnlyQuantizer(Quantizer):
    def quantize(
        self, model: torch.nn.Module, *args: Any, **kwargs: Any
    ) -> torch.nn.Module:
        apply_weight_only_int8_quant(model)
        return model


_quantizer_to_mode = {
    Int4WeightOnlyQuantizer: "4w",
    Int8WeightOnlyQuantizer: "8w",
    Int4WeightOnlyGPTQQuantizer: "4w-gptq",
    # Int8DynActInt4WeightQuantizer: "8da4w",
}


def get_quantizer_mode(quantizer: Optional[Callable]) -> Optional[str]:
    """Given a quantizer object, returns a string that specifies the type of quantization e.g.
    4w, which means int4 weight only quantization.
    If the quantizer is not recognized as a known quantizer, we'll return None
    """
    return _quantizer_to_mode.get(type(quantizer), None)


2. Download config files to some local repo to customize
3. Download the models
4. Run the config commands

# Models

These are the steps to execute for each model. 

## Llama3-70b

### config
https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama3/70B_lora.yaml

### download
tune download meta-llama/Meta-Llama-3-70B-Instruct \
--output-dir /data2/fabricehc/overdose/.cache/Meta-Llama-3-70B-Instruct \
--hf-token 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 \
--ignore-patterns "original/consolidated*"

### finetune
tune run --master_port 29500 --nproc_per_node 3 lora_finetune_distributed --config /data2/fabricehc/impossibility-watermark/oracles/training/torchtune/configs/llama3_1_70B.yaml

### convert to hf format
https://github.com/pytorch/torchtune/issues/878 # rename file paths in `model.safetensors.index.json`

### upload to hf -- huggingface-cli upload <hf-repo-id> <checkpoint-dir>
huggingface-cli upload fabriceyhc/Meta-Llama-3-70B-Instruct-DrugDetection-v3 /data2/fabricehc/overdose/.cache/Meta-Llama-3-70B-Instruct-DrugDetection-v3-hf

## Llama3-8B

### config
https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama3/8B_lora.yaml

### download
tune download meta-llama/Meta-Llama-3-8B-Instruct \
--output-dir /data2/fabricehc/overdose/.cache/Meta-Llama-3-8B-Instruct \
--hf-token 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

### finetune
tune run --master_port 29501 --nproc_per_node 1 lora_finetune_distributed --config /data2/fabricehc/overdose/configs/llama3/8B_lora.yaml

### generate
tune run generate --config /data2/fabricehc/overdose/configs/llama3/8B_lora_generate.yaml

### convert to hf format
#### NOTE: move all the files from /original/ into the input_dir + run from the most recently installed pip version of huggingface (currently set up in /tmp/transformers)
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
--input_dir /data2/fabricehc/overdose/.cache/Meta-Llama-3-8B-Instruct-DrugDetection-v3 \
--llama_version 3 \
--model_size 8B \
--output_dir /data2/fabricehc/overdose/.cache/Meta-Llama-3-8B-Instruct-DrugDetection-v3-hf 

### upload to hf -- huggingface-cli upload <hf-repo-id> <checkpoint-dir>
huggingface-cli upload fabriceyhc/Meta-Llama-3-8B-Instruct-DrugDetection-v3 /data2/fabricehc/overdose/.cache/Meta-Llama-3-8B-Instruct-DrugDetection-v3-hf


# Llama-3.1-70B-Instruct-IMP-DiffOracle

## finetune
CUDA_VISIBLE_DEVICES=5,6,7 tune run --master_port 29500 --nproc_per_node 3 lora_finetune_distributed --config /data2/fabricehc/impossibility-watermark/oracles/training/torchtune/configs/llama3_1_70B_IMP.yaml

## convert 30 shards into single safetensors model
python -m oracles.training.torchtune.convert

## move model.safetensors into dedicated repository with all the required config files (e.g. tokenizer.model, etc)

## convert hf to gguf with llama.cpp
python convert_hf_to_gguf.py /data2/.shared_models/models--meta-llama--Meta-Llama-3.1-70B-Instruct-IMP-0.1-hf --outfile /data2/.shared_models/llama.cpp_models/Meta-Llama-3.1-70B-Instruct-IMP-0.1-q8_0.gguf --outtype q8_0

# Llama-3.1-70B-Instruct-IMP-MutationOracle

# add new custom loader class to torchtune/torchtune/datasets/_chat.py
# reference class in torchtune/torchtune/datasets/__init__.py
# create custom yaml config for torchtune and reference class, e.g. impossibility-watermark/oracles/training/torchtune/configs/llama3_1_70B_IMP_mutationoracle.yaml
#   also create unique folder name for trained model output
# find server with 240+ GB of GPU memory for training the 70B model

## finetune
CUDA_VISIBLE_DEVICES=1,2,3 tune run --master_port 29500 --nproc_per_node 3 lora_finetune_distributed --config /data2/fabricehc/impossibility-watermark/oracles/training/torchtune/configs/llama3_1_70B_IMP_mutationoracle.yaml

## convert 30 shards into single safetensors model
python -m oracles.training.torchtune.convert

## move model.safetensors into dedicated repository with all the required config files (e.g. tokenizer.model, etc)

## convert hf to gguf with llama.cpp
python convert_hf_to_gguf.py /data2/.shared_models/models--meta-llama--Meta-Llama-3.1-70B-Instruct-IMP-MutationOracle-0.1-hf --outfile /data2/.shared_models/llama.cpp_models/Meta-Llama-3.1-70B-Instruct-IMP-MutationOracle-0.1-q8_0.gguf --outtype q8_0

# Llama-3.1-70B-Instruct-IMP3

# add new custom loader class to torchtune/torchtune/datasets/_chat.py
# reference class in torchtune/torchtune/datasets/__init__.py
# create custom yaml config for torchtune and reference class, e.g. impossibility-watermark/oracles/training/torchtune/configs/llama3_1_70B_IMP3.yaml
#   also create unique folder name for trained model output
# find server with 240+ GB of GPU memory for training the 70B model

## finetune
CUDA_VISIBLE_DEVICES=1,2,3 tune run --master_port 29500 --nproc_per_node 3 lora_finetune_distributed --config /data2/fabricehc/impossibility-watermark/oracles/training/torchtune/configs/llama3_1_70B_IMP3.yaml

## convert 30 shards into single safetensors model
python -m oracles.training.torchtune.convert

## move model.safetensors into dedicated repository with all the required config files (e.g. tokenizer.model, etc)

## convert hf to gguf with llama.cpp
python convert_hf_to_gguf.py /data2/.shared_models/models--meta-llama--Meta-Llama-3.1-70B-Instruct-IMP3-0.1-hf --outfile /data2/.shared_models/llama.cpp_models/Meta-Llama-3.1-70B-Instruct-IMP3-0.1-q8_0.gguf --outtype q8_0