results_dir: "./oracles/training/setfit/results/"

dataset:
    train_data_path: "./data/WQE/train.csv"
    val_data_path: "./data/WQE/val.csv"
    test_data_path: "./data/WQE/test.csv"
    max_val_size: 150

model:
    name: "microsoft/deberta-v3-large"
    cache_dir: "/data2/.shared_models/"

trainer:
    output_dir: "${results_dir}/finetuned_models/${model.name}-WQE-0.3"
    cuda_visible_devices: "0"
    device_map: "auto"
    num_train_epochs: 20
    max_steps: 100000
    logging_steps: 1000
    evaluation_strategy: "steps"
    save_strategy: "steps"
    save_steps: ${trainer.logging_steps}
    save_total_limit: 1
    load_best_model_at_end: True
    early_stopping_patience: 10
    seed: 42
    trust_remote_code: True
    use_amp: False
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 1
    fp16_full_eval: True
    eval_accumulation_steps: 1
    eval_steps: -1
    gradient_checkpointing: True
    max_grad_norm: 0.3
    learning_rate: 2e-4
    weight_decay: 0.001
    optim: "adamw_torch"
    lr_scheduler_type: "constant"
    warmup_ratio: 0.03
    warmup_steps: 10
    group_by_length: True
    max_seq_length: 3072 
    fp16: True
    bf16: True
    tf32: True
    packing: True
    eval_packing: False
    dataset_num_proc: 10
    # lora parameters
    lora_r: 16
    lora_alpha: 8
    lora_dropout: 0.05
    lora_task_type: "CAUSAL_LM"
    lora_bias: "none"
    use_gradient_checkpointing: True
    lora_target_modules: "all-linear"
    # bitsandbytes parameters
    use_bitsandbytes: False
    use_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_quant_type: "nf4"
    use_nested_quant: False
    # FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
    fsdp: "full_shard auto_wrap" # offload" # remove offload if enough GPU memory
    backward_prefetch: "backward_pre"
    forward_prefetch: "false"
    use_orig_params: "false"
    annotation_path: "${results_dir}/annotations/${model.name}_WQE.csv"