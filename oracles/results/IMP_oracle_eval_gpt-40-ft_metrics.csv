oracle_type,oracle_class,judge_name,explain,average_time_taken,average_precision,average_recall,average_f1_score,TP_A_BETTER,TN_A_BETTER,FP_A_BETTER,FN_A_BETTER,support_A_BETTER,precision_A_BETTER,recall_A_BETTER,f1_score_A_BETTER,TP_B_BETTER,TN_B_BETTER,FP_B_BETTER,FN_B_BETTER,support_B_BETTER,precision_B_BETTER,recall_B_BETTER,f1_score_B_BETTER,TP_TIE,TN_TIE,FP_TIE,FN_TIE,support_TIE,precision_TIE,recall_TIE,f1_score_TIE
guidance,DiffOracle,ft:gpt-4o-2024-08-06:ucla-pluslab:imp-binary-diff-mutation:A0kklWac,False,0.46121653453076733,0.7732519162743884,0.7740585774058577,0.7731588967519638,111.0,74.0,30.0,24.0,135.0,0.7872340425531915,0.8222222222222222,0.8043478260869565,74.0,111.0,24.0,30.0,104.0,0.7551020408163265,0.7115384615384616,0.7326732673267328,0.0,239.0,0.0,0.0,0.0,0.0,0.0,0.0
guidance,MutationOracle,ft:gpt-4o-2024-08-06:ucla-pluslab:imp-binary-diff-mutation:A0kklWac,False,0.8375154459303009,0.7736357958264393,0.7740585774058577,0.7737924413783874,109.0,76.0,28.0,26.0,135.0,0.7956204379562044,0.8074074074074075,0.8014705882352942,76.0,109.0,26.0,28.0,104.0,0.7450980392156863,0.7307692307692307,0.7378640776699028,0.0,239.0,0.0,0.0,0.0,0.0,0.0,0.0
guidance,BinaryOracle,ft:gpt-4o-2024-08-06:ucla-pluslab:imp-binary-diff-mutation:A0kklWac,False,0.3687281049943868,0.7985908865460187,0.799163179916318,0.7983634637795233,114.0,77.0,27.0,21.0,135.0,0.8085106382978723,0.8444444444444444,0.8260869565217391,77.0,114.0,21.0,27.0,104.0,0.7857142857142857,0.7403846153846154,0.7623762376237625,0.0,239.0,0.0,0.0,0.0,0.0,0.0,0.0
