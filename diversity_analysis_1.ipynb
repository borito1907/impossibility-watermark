{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /home/borito1907/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from diversity_oracle import DiversityOracle\n",
    "from utils import save_to_csv, find_csv, count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_oracle = DiversityOracle(metrics = {}, verbose=False, normalized=False)\n",
    "normalized_div_oracle = DiversityOracle(metrics = {}, verbose=False, normalized=True)\n",
    "\n",
    "def get_success_dfs(csv_files):\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df[df['quality_preserved'] == True])\n",
    "    return dfs\n",
    "\n",
    "def create_corpuses(dfs, normalized):\n",
    "    corpuses = []\n",
    "    min_length = min(len(df) for df in dfs)\n",
    "    \n",
    "    if normalized:\n",
    "        for i in range(min_length):\n",
    "            corpus = [df.iloc[i]['mutated_text'] for df in dfs]\n",
    "            corpuses.append(corpus)\n",
    "    else:\n",
    "        for i in range(min_length):\n",
    "            corpus = [text for df in dfs for text in df.iloc[:i+1]['mutated_text']]\n",
    "            corpuses.append(corpus)\n",
    "    return corpuses\n",
    "\n",
    "def get_diversity_df(csv_files, normalized):\n",
    "    dfs = get_success_dfs(csv_files)\n",
    "    corpuses = create_corpuses(dfs, normalized)\n",
    "    metric_dicts = []\n",
    "    for corpus in corpuses:\n",
    "        if normalized:\n",
    "            metrics = normalized_div_oracle(corpus)\n",
    "        else:\n",
    "            metrics = div_oracle(corpus)\n",
    "        metric_dict = {metric['metric_name']: metric['diversity_score'] for metric in metrics}\n",
    "        \n",
    "        metric_dicts.append(metric_dict)\n",
    "    \n",
    "    df = pd.DataFrame(metric_dicts)\n",
    "    return df\n",
    "\n",
    "def plot_metric(df, column_name):\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size (optional)\n",
    "    plt.plot(df.index, df[column_name], marker='o', linestyle='-', color='b')  # Plot with line and markers\n",
    "    plt.title(f\"Evolution of {column_name}\")  # Title of the plot\n",
    "    plt.xlabel('Step Number')  # X-axis label\n",
    "    plt.ylabel(column_name)  # Y-axis label, replace with your column name\n",
    "    plt.grid(True)  # Show grid\n",
    "    plt.show()\n",
    "\n",
    "def save_plots(df, folder):\n",
    "    for column_name in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df.index, df[column_name], marker='o', linestyle='-', color='b')\n",
    "        plt.title(f'Evolution of {column_name} Values Over Rows')\n",
    "        plt.xlabel('Step Number')\n",
    "        plt.ylabel(column_name)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the figure as a PNG file\n",
    "        filename = f'{folder}/{column_name}.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        \n",
    "        # Clear the current figure to avoid overlapping of plots\n",
    "        plt.clf()  # Use plt.close() if you want to close the figure completely       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalized Values\n",
    "txt_file_directory = \"./third_round/\"\n",
    "plots_folder = \"./plots/plots_3/\"\n",
    "\n",
    "for txt_filename in os.listdir(txt_file_directory):\n",
    "    print(f\"Filename: {txt_filename}\")\n",
    "    csv_filename = find_csv(txt_filepath)\n",
    "    directory = \"./eval/results/\"\n",
    "    csv_filepath = os.path.join(directory, csv_filename)\n",
    "\n",
    "    csv_files = [csv_filepath]\n",
    "    \n",
    "    div_df = get_diversity_df(csv_files, True)\n",
    "    \n",
    "    directory_path = os.path.join(plots_folder, txt_filename[:-4])\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "    csv_path = os.path.join(directory_path, 'normalized_div_df.csv')\n",
    "\n",
    "    save_to_csv(div_df, csv_path)\n",
    "    save_plots(div_df, directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-normalized Values\n",
    "txt_file_directory = \"./third_round/\"\n",
    "plots_folder = \"./plots/plots_3/\"\n",
    "\n",
    "for txt_filename in os.listdir(txt_file_directory):\n",
    "    print(f\"Filename: {txt_filename}\")\n",
    "    txt_filepath = os.path.join(txt_file_directory, txt_filename)\n",
    "    csv_filename = find_csv(txt_filepath)\n",
    "    directory = \"./eval/results/\"\n",
    "    csv_filepath = os.path.join(directory, csv_filename)\n",
    "\n",
    "    csv_files = [csv_filepath]\n",
    "    \n",
    "    div_df = get_diversity_df(csv_files, False)\n",
    "    \n",
    "    directory_path = os.path.join(plots_folder, txt_filename[:-4])\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "    csv_path = os.path.join(directory_path, 'div_df.csv')\n",
    "\n",
    "    save_to_csv(div_df, csv_path)\n",
    "    save_plots(div_df, directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann-Whitney U-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def compute_mann_whitney_u(scores, window1_size, window2_size):\n",
    "    \"\"\"\n",
    "    Computes the Mann-Whitney U test statistic and p-value between two sliding windows of scores.\n",
    "    \n",
    "    Args:\n",
    "        scores (list or np.array): A list or array of scores.\n",
    "        window1_size (int): The size of the first window.\n",
    "        window2_size (int): The size of the second window.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the U statistic and the p-value.\n",
    "    \"\"\"\n",
    "    # Ensure that the input is a numpy array\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    # Compute the number of scores\n",
    "    num_scores = len(scores)\n",
    "    \n",
    "    # Check if the windows are valid\n",
    "    if window1_size + window2_size != num_scores:\n",
    "        raise ValueError(\"The sum of the window sizes should be equal to the number of scores.\")\n",
    "    \n",
    "    # Define the two windows\n",
    "    window1 = scores[:window1_size]\n",
    "    window2 = scores[window2_size:]\n",
    "    \n",
    "    # Compute the Mann-Whitney U test\n",
    "    u_stat, p_val = mannwhitneyu(window1, window2, alternative='two-sided')\n",
    "    \n",
    "    return u_stat, p_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
