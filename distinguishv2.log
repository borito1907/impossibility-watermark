[2024-04-25 17:23:40,663][__main__][INFO] - Initializing a new Distinguisher pipeline from cfg...
[2024-04-25 17:23:40,663][model_builders.pipeline][INFO] - Initializing TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
[2024-04-25 17:23:41,328][datasets][INFO] - PyTorch version 2.2.1 available.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/distinguish.py", line 126, in test
    distinguisher = Distinguisher(cfg.generator_args)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/distinguish.py", line 56, in __init__
    self.pipeline = PipeLineBuilder(self.cfg)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/model_builders/pipeline.py", line 36, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3829, in from_pretrained
    model = quantizer.post_init_model(model)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/optimum/gptq/quantizer.py", line 598, in post_init_model
    model = autogptq_post_init(model, use_act_order=self.desc_act)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/AutoGPTQ/auto_gptq/modeling/_utils.py", line 450, in autogptq_post_init
    "temp_dq": torch.zeros(
               ^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 3 has a total capacity of 47.33 GiB of which 58.81 MiB is free. Process 3574081 has 44.81 GiB memory in use. Including non-PyTorch memory, this process has 2.44 GiB memory in use. Of the allocated memory 2.13 GiB is allocated by PyTorch, and 12.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
