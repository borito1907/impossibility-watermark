/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[2024-09-05 04:23:17,861][__main__][INFO] - Getting the watermarker...
[2024-09-05 04:23:17,861][watermarker][INFO] - Using device: cuda:0
[2024-09-05 04:23:17,861][model_builders.pipeline][INFO] - Initializing hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
[2024-09-05 04:23:18,791][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]