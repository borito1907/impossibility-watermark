{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is THAT what you mean, Mrs. Hussey?']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/34805790/how-to-avoid-nltks-sentence-tokenizer-splitting-on-abbreviations\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "punkt_param = PunktParameters()\n",
    "punkt_param.abbrev_types = set(['dr', 'vs', 'mr', 'mrs', 'prof', 'inc'])\n",
    "sentence_splitter = PunktSentenceTokenizer(punkt_param)\n",
    "text = \"is THAT what you mean, Mrs. Hussey?\"\n",
    "sentences = sentence_splitter.tokenize(text)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# from watermarkers.SemStamp.sampling_utils import tokenize_sentences\n",
    "\n",
    "import re\n",
    "from typing import *\n",
    "\n",
    "def handle_bullet_points(sentences):\n",
    "    new_sentences = []\n",
    "    digit_pattern = re.compile(r'^\\d+\\.$')\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    num_sentences = len(sentences)\n",
    "    # print(f\"Num sentences: {num_sentences}\")\n",
    "\n",
    "    while i < num_sentences - 1:\n",
    "        if digit_pattern.match(sentences[i].strip()):\n",
    "            modified_sentence = f\"{sentences[i].strip()} {sentences[i + 1]}\"\n",
    "            new_sentences.append(modified_sentence)\n",
    "            # print(f\"Adding {modified_sentence}\")\n",
    "            i += 1  # Skip the next element as it's already added\n",
    "        else:\n",
    "            new_sentences.append(sentences[i])\n",
    "        i += 1\n",
    "        # print(f\"i={i}\")\n",
    "\n",
    "    # Add the last sentence as well, if we don't want to skip it\n",
    "    if i == num_sentences - 1:\n",
    "        new_sentences.append(sentences[-1])\n",
    "    \n",
    "    return new_sentences\n",
    "\n",
    "def tokenize_sentences(text: str) -> List[str]:\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = handle_bullet_points(sentences)\n",
    "    return processed_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They work with their colleagues in the Senate and the House of Representatives to draft, debate, and pass legislation.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"They work with their colleagues in the Senate and the House of Representatives to draft, debate, and pass legislation.\"\n",
    "\n",
    "sents = sent_tokenize(txt)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"As a helpful personal assistant, a United States Senator has several key responsibilities that shape the country's laws, policies, and direction.\",\n",
       " 'Here are the main responsibilities of an American Senator:\\n\\n1.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"As a helpful personal assistant, a United States Senator has several key responsibilities that shape the country's laws, policies, and direction. Here are the main responsibilities of an American Senator:\n",
    "\n",
    "1.\"\"\"\n",
    "\n",
    "sents = tokenize_sentences(txt)\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2. Boran']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"2. Boran\"\"\"\n",
    "\n",
    "sents = tokenize_sentences(txt)\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Apples\\n2.', 'Bananas\\n3.', 'Milk\\n4.', 'Bread\\n5.', 'Eggs']\n",
      "['1. Welcome and Introductions\\n2. Review of Previous Meeting Minutes\\n3.', 'Project Updates\\n4.', 'Budget Review\\n5.', 'Q&A Session']\n",
      "['1. Preheat the oven to 350째F (175째C).', '2. Grease and flour a 9x9 inch pan.', '3. In a medium bowl, mix together flour, sugar, and baking powder.', '4. Add eggs, milk, and butter; beat until smooth.', '5. Pour batter into the prepared pan and bake for 30-35 minutes.']\n",
      "['1. High-resolution display\\n2.', 'Long-lasting battery life\\n3.', 'Fast processor\\n4.', 'Multiple camera lenses\\n5.', '5G connectivity']\n"
     ]
    }
   ],
   "source": [
    "sample_text_1 = \"\"\"\n",
    "1. Apples\n",
    "2. Bananas\n",
    "3. Milk\n",
    "4. Bread\n",
    "5. Eggs\n",
    "\"\"\"\n",
    "\n",
    "sample_text_2 = \"\"\"\n",
    "1. Welcome and Introductions\n",
    "2. Review of Previous Meeting Minutes\n",
    "3. Project Updates\n",
    "4. Budget Review\n",
    "5. Q&A Session\n",
    "\"\"\"\n",
    "\n",
    "sample_text_3 = \"\"\"\n",
    "1. Preheat the oven to 350째F (175째C).\n",
    "2. Grease and flour a 9x9 inch pan.\n",
    "3. In a medium bowl, mix together flour, sugar, and baking powder.\n",
    "4. Add eggs, milk, and butter; beat until smooth.\n",
    "5. Pour batter into the prepared pan and bake for 30-35 minutes.\n",
    "\"\"\"\n",
    "\n",
    "sample_text_4 = \"\"\"\n",
    "1. High-resolution display\n",
    "2. Long-lasting battery life\n",
    "3. Fast processor\n",
    "4. Multiple camera lenses\n",
    "5. 5G connectivity\n",
    "\"\"\"\n",
    "\n",
    "sents = tokenize_sentences(sample_text_1)\n",
    "print(sents)\n",
    "\n",
    "sents = tokenize_sentences(sample_text_2)\n",
    "print(sents)\n",
    "\n",
    "sents = tokenize_sentences(sample_text_3)\n",
    "print(sents)\n",
    "\n",
    "sents = tokenize_sentences(sample_text_4)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F.FF\n",
      "======================================================================\n",
      "FAIL: test_bullet_points_with_different_formats (__main__.TestTokenizeSentences)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1585470/1629283226.py\", line 22, in test_bullet_points_with_different_formats\n",
      "    self.assertEqual(result, expected)\n",
      "AssertionError: Lists differ: ['1. [13 chars]point\\n        2) Second bullet point\\n       [84 chars]int'] != ['1. [13 chars]point', '2) Second bullet point', 'a. Third bu[57 chars]int']\n",
      "\n",
      "First differing element 0:\n",
      "'1. First bullet point\\n        2) Second bullet point\\n        a.'\n",
      "'1. First bullet point'\n",
      "\n",
      "Second list contains 3 additional elements.\n",
      "First extra element 2:\n",
      "'a. Third bullet point'\n",
      "\n",
      "- ['1. First bullet point\\n        2) Second bullet point\\n        a.',\n",
      "+ ['1. First bullet point',\n",
      "+  '2) Second bullet point',\n",
      "-  'Third bullet point\\n'\n",
      "?                     --\n",
      "\n",
      "+  'a. Third bullet point',\n",
      "?   +++                   +\n",
      "\n",
      "-  '        b) Fourth bullet point\\n'\n",
      "?   --------                      --\n",
      "\n",
      "+  'b) Fourth bullet point',\n",
      "?                          +\n",
      "\n",
      "-  '        - Fifth bullet point']\n",
      "?   --------\n",
      "\n",
      "+  '- Fifth bullet point']\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_mixed_bullet_points_and_regular_sentences (__main__.TestTokenizeSentences)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1585470/1629283226.py\", line 35, in test_mixed_bullet_points_and_regular_sentences\n",
      "    self.assertEqual(result, expected)\n",
      "AssertionError: Lists differ: ['\\n        This is a regular sentence.', '[85 chars]nt.'] != ['This is a regular sentence.', '1. This is[72 chars]nt.']\n",
      "\n",
      "First differing element 0:\n",
      "'\\n        This is a regular sentence.'\n",
      "'This is a regular sentence.'\n",
      "\n",
      "First list contains 1 additional elements.\n",
      "First extra element 2:\n",
      "'This is another regular sentence following a bullet point.'\n",
      "\n",
      "- ['\\n        This is a regular sentence.',\n",
      "?   ----------\n",
      "\n",
      "+ ['This is a regular sentence.',\n",
      "-  '1. This is a bullet point.',\n",
      "-  'This is another regular sentence following a bullet point.']\n",
      "+  '1. This is a bullet point. This is another regular sentence following a '\n",
      "+  'bullet point.']\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_single_bullet_point_with_trailing_sentence (__main__.TestTokenizeSentences)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1585470/1629283226.py\", line 46, in test_single_bullet_point_with_trailing_sentence\n",
      "    self.assertEqual(result, expected)\n",
      "AssertionError: Lists differ: ['1. [14 chars]point\\n        Trailing sentence not part of t[13 chars]nt.'] != ['1. [14 chars]point Trailing sentence not part of the bullet point.']\n",
      "\n",
      "First differing element 0:\n",
      "'1. S[13 chars]point\\n        Trailing sentence not part of the bullet point.'\n",
      "'1. S[13 chars]point Trailing sentence not part of the bullet point.'\n",
      "\n",
      "- ['1. Single bullet point\\n'\n",
      "-  '        Trailing sentence not part of the bullet point.']\n",
      "? ^ ^^^^^^^\n",
      "\n",
      "+ ['1. Single bullet point Trailing sentence not part of the bullet point.']\n",
      "? ^ ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.006s\n",
      "\n",
      "FAILED (failures=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=3>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define the test cases\n",
    "class TestTokenizeSentences(unittest.TestCase):\n",
    "\n",
    "    def test_bullet_points_with_different_formats(self):\n",
    "        text = \"\"\"\n",
    "        1. First bullet point\n",
    "        2) Second bullet point\n",
    "        a. Third bullet point\n",
    "        b) Fourth bullet point\n",
    "        - Fifth bullet point\n",
    "        \"\"\"\n",
    "        expected = [\n",
    "            \"1. First bullet point\",\n",
    "            \"2) Second bullet point\",\n",
    "            \"a. Third bullet point\",\n",
    "            \"b) Fourth bullet point\",\n",
    "            \"- Fifth bullet point\"\n",
    "        ]\n",
    "        result = tokenize_sentences(text)\n",
    "        self.assertEqual(result, expected)\n",
    "    \n",
    "    def test_mixed_bullet_points_and_regular_sentences(self):\n",
    "        text = \"\"\"\n",
    "        This is a regular sentence.\n",
    "        1. This is a bullet point.\n",
    "        This is another regular sentence following a bullet point.\n",
    "        \"\"\"\n",
    "        expected = [\n",
    "            \"This is a regular sentence.\",\n",
    "            \"1. This is a bullet point. This is another regular sentence following a bullet point.\"\n",
    "        ]\n",
    "        result = tokenize_sentences(text)\n",
    "        self.assertEqual(result, expected)\n",
    "    \n",
    "    def test_single_bullet_point_with_trailing_sentence(self):\n",
    "        text = \"\"\"\n",
    "        1. Single bullet point\n",
    "        Trailing sentence not part of the bullet point.\n",
    "        \"\"\"\n",
    "        expected = [\n",
    "            \"1. Single bullet point Trailing sentence not part of the bullet point.\"\n",
    "        ]\n",
    "        result = tokenize_sentences(text)\n",
    "        self.assertEqual(result, expected)\n",
    "    \n",
    "    def test_bullet_points_with_punctuation(self):\n",
    "        text = \"\"\"\n",
    "        1. This is the first bullet point.\n",
    "        2. This is the second bullet point; with more text.\n",
    "        3. Third bullet point: continues here.\n",
    "        \"\"\"\n",
    "        expected = [\n",
    "            \"1. This is the first bullet point.\",\n",
    "            \"2. This is the second bullet point; with more text.\",\n",
    "            \"3. Third bullet point: continues here.\"\n",
    "        ]\n",
    "        result = tokenize_sentences(text)\n",
    "        self.assertEqual(result, expected)\n",
    "\n",
    "# Run the tests\n",
    "unittest.TextTestRunner().run(unittest.makeSuite(TestTokenizeSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**Positive Impacts:**\\n\\n1.',\n",
       " '**Job Creation:** A climate-focused bill could lead to an increase in jobs related to renewable energy, sustainable infrastructure, to green technology, and environmental conservation.',\n",
       " 'This could boost local employment rates and stimulate economic growth.',\n",
       " '2. **Investment Attraction:** Bipartisan support for climate action can attract investors seeking to capitalize on emerging clean technologies and sustainable industries.',\n",
       " 'This influx of capital can revitalize local economies and create new business opportunities.',\n",
       " '3. **Infrastructure Development:** Climate-resilient infrastructure projects, such as sea walls, levees, and green roofs can generate construction jobs and stimulate local spending.',\n",
       " '4. **Innovation Hubs:** Regions with strong research institutions or existing cleantech industries may become hubs for innovation, to climate-related R&D, driving economic growth through knowledge-based entrepreneurship.',\n",
       " '**Challenges and Risks:**\\n\\n1.',\n",
       " '**Transition Costs:** The shift away from fossil fuels and towards cleaner energy sources can result in short-term job losses and economic disruption in regions heavily reliant on traditional energy industries.',\n",
       " '2. **Regulatory Burden:** Stricter environmental regulations may increase compliance costs for businesses, potentially affecting their competitiveness and profitability.',\n",
       " '3.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentences(\"\"\"**Positive Impacts:**\n",
    "\n",
    "1. **Job Creation:** A climate-focused bill could lead to an increase in jobs related to renewable energy, sustainable infrastructure, to green technology, and environmental conservation. This could boost local employment rates and stimulate economic growth.\n",
    "2. **Investment Attraction:** Bipartisan support for climate action can attract investors seeking to capitalize on emerging clean technologies and sustainable industries. This influx of capital can revitalize local economies and create new business opportunities.\n",
    "3. **Infrastructure Development:** Climate-resilient infrastructure projects, such as sea walls, levees, and green roofs can generate construction jobs and stimulate local spending.\n",
    "4. **Innovation Hubs:** Regions with strong research institutions or existing cleantech industries may become hubs for innovation, to climate-related R&D, driving economic growth through knowledge-based entrepreneurship.\n",
    "\n",
    "**Challenges and Risks:**\n",
    "\n",
    "1. **Transition Costs:** The shift away from fossil fuels and towards cleaner energy sources can result in short-term job losses and economic disruption in regions heavily reliant on traditional energy industries.\n",
    "2. **Regulatory Burden:** Stricter environmental regulations may increase compliance costs for businesses, potentially affecting their competitiveness and profitability.\n",
    "3.\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
