/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[2024-05-26 12:26:56,507][__main__][INFO] - Starting to watermark...
[2024-05-26 12:26:56,510][__main__][INFO] - Prompt: Discuss the implications of a bipartisan bill on climate change for local economies.
[2024-05-26 12:26:56,510][__main__][INFO] - Getting the watermarker...
[2024-05-26 12:26:56,510][watermarker][INFO] - Using device: cuda
[2024-05-26 12:26:56,510][model_builders.pipeline][INFO] - Initializing MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ
/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.
INFO - The layer lm_head is not quantized.
[2024-05-26 12:26:59,883][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2024-05-26 12:27:07,681][auto_gptq.utils.accelerate_utils][WARNING] - Some weights of the model checkpoint at ./local1/borito1907/.cache/models--MaziyarPanahi--Meta-Llama-3-70B-Instruct-GPTQ/snapshots/46c7afccd4f9345a3d43c1468fde1034cf0a0932/model.safetensors were not used when initializing LlamaForCausalLM: set(). This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.
  warn_deprecated(
[2024-05-26 12:27:09,385][semstamp][INFO] - Setting up generating components...
[2024-05-26 12:27:09,388][semstamp][INFO] - {'max_new_tokens': 1024, 'do_sample': True, 'temperature': 1.7, 'top_p': 0.95, 'top_k': 40, 'repetition_penalty': 1.1, 'bad_words_ids': [[198]], 'min_new_tokens': 245}
[2024-05-26 12:27:09,388][semstamp][INFO] - Initializing embedder model.
[2024-05-26 12:27:09,388][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v1
/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2024-05-26 12:27:10,222][sentence_transformers.SentenceTransformer][INFO] - Use pytorch device_name: cuda
[2024-05-26 12:27:10,282][semstamp][INFO] - Finished initializing embedder model.
initializing random projection LSH model
loading SBERT base model...
[2024-05-26 12:27:10,283][__main__][INFO] - Got the watermarker. Generating watermarked text...
[2024-05-26 12:27:10,283][semstamp][INFO] - Passing the following prompt to the LSH reject completion function:
 Discuss the implications of a bipartisan bill on climate change for local economies.
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
[2024-05-26 12:27:13,508][semstamp][INFO] - Candidate text: 

A bipartisan bill on climate change could have significant implications for local economies across the United States. Here are some potential effects to consider:

**Positive Impacts:**

1.
[2024-05-26 12:27:13,509][semstamp][INFO] - Accept Mask: tensor([7, 2], device='cuda:0')
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00, 140.59it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00, 153.42it/s]
[2024-05-26 12:27:13,530][semstamp][INFO] - LSH Candidate: 0
[2024-05-26 12:27:13,530][semstamp][INFO] - Candidate text is doesn't fall into the correct place in the embedding space.
[2024-05-26 12:27:13,530][semstamp][INFO] - acceptmasklist: [7, 2]
[2024-05-26 12:27:13,530][semstamp][INFO] - acceptmaskstr: 7,2
Error executing job with overrides: ['++attack_args.prompt_file=./inputs/tests_v1_with_lotr.csv', '++attack_args.prompt_num=2', '++attack_args.is_completion=False', '++generator_args.temperature=1.7', '++generator_args.diversity_penalty=5', '++generator_args.generation_stats_csv_path=prompt_based_stats/prompt_based_2_temp_170_divp_5_attempt_3_stats.csv', '++watermark_args.save_file_name=prompt_based_saves/prompt_based_2_temp_170_divp_5_attempt_3.csv']
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/watermarked_text_generator.py", line 27, in test
    watermarked_text = watermarker.generate(prompt)
  File "/local1/borito1907/impossibility-watermark/watermarker.py", line 51, in generate
    completion = self.generate_watermarked_outputs(prompt)
  File "/local1/borito1907/impossibility-watermark/semstamp.py", line 269, in generate_watermarked_outputs
    return self._lsh_generate_watermarked_outputs(prompt)
  File "/local1/borito1907/impossibility-watermark/semstamp.py", line 281, in _lsh_generate_watermarked_outputs
    response = self.lsh_reject_completion(prompt, stats_csv_path = self.cfg.generator_args.generation_stats_csv_path)
  File "/local1/borito1907/impossibility-watermark/semstamp.py", line 235, in lsh_reject_completion
    save_to_csv_with_filepath([stats], stats_csv_path)
  File "/local1/borito1907/impossibility-watermark/utils.py", line 29, in save_to_csv_with_filepath
    df_out.to_csv(file_path, index=False)  # Create new file with headers
  File "/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/pandas/core/generic.py", line 3466, in to_csv
    return DataFrameRenderer(formatter).to_csv(
  File "/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/pandas/io/formats/csvs.py", line 237, in save
    with get_handle(
  File "/local1/borito1907/anaconda3/envs/watermark_2/lib/python3.10/site-packages/pandas/io/common.py", line 702, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'prompt_based_stats/prompt_based_2_temp_170_divp_5_attempt_3_stats.csv'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
