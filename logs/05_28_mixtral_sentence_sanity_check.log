/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
CUDA extension not installed.
CUDA extension not installed.
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/runpy.py:126: RuntimeWarning: 'mutators.sentence' found in sys.modules after import of package 'mutators', but prior to execution of 'mutators.sentence'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
CUDA_VISIBLE_DEVICES: 1,2
WORLD_SIZE: 2
{'type': 'sentence', 'use_system_profile': True, 'system_profile': 'You are a copy editor tasked to enforce text quality.', 'num_retries': 5, 'model_name_or_path': 'TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', 'model_cache_dir': '${model_cache_dir}', 'revision': 'main', 'device_map': '${generator_args.device_map}', 'trust_remote_code': '${generator_args.trust_remote_code}', 'max_new_tokens': '${generator_args.max_new_tokens}', 'do_sample': '${generator_args.do_sample}', 'temperature': '${generator_args.temperature}', 'top_p': '${generator_args.top_p}', 'top_k': '${generator_args.top_k}', 'repetition_penalty': '${generator_args.repetition_penalty}', 'use_pydantic_parser': True}
[2024-05-28 12:10:57,943][__main__][INFO] - Type of pipeline was: <class 'NoneType'>
[2024-05-28 12:10:57,943][__main__][INFO] - Initializing a new Text Mutator pipeline from cfg...
[2024-05-28 12:10:57,943][model_builders.pipeline][INFO] - Initializing TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
[2024-05-28 12:10:59,781][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2024-05-28 12:11:09,929][__main__][INFO] - Sentence to rephrase: The One Ring represents the ultimate form of power, as it allows its possessor to dominate and rule over the entire world.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Traceback (most recent call last):
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/local1/borito1907/impossibility-watermark/mutators/sentence.py", line 165, in <module>
    test()
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/local1/borito1907/impossibility-watermark/mutators/sentence.py", line 156, in test
    mutated_text = text_mutator.mutate(text)
  File "/local1/borito1907/impossibility-watermark/mutators/sentence.py", line 115, in mutate
    mutated_analysis = self.rephrase_sentence(text)
  File "/local1/borito1907/impossibility-watermark/mutators/sentence.py", line 93, in rephrase_sentence
    rephrased_sentence = self.chain.invoke(dict_input)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 2499, in invoke
    input = step.invoke(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 276, in invoke
    self.generate_prompt(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 633, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 803, in generate
    output = self._generate_helper(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 670, in _generate_helper
    raise e
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 657, in _generate_helper
    self._generate(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py", line 267, in _generate
    responses = self.pipeline(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/pipelines/text_generation.py", line 240, in __call__
    return super().__call__(text_inputs, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/pipelines/base.py", line 1223, in __call__
    outputs = list(final_iterator)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py", line 124, in __next__
    item = next(self.iterator)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py", line 125, in __next__
    processed = self.infer(item, **self.params)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/pipelines/base.py", line 1149, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/pipelines/text_generation.py", line 327, in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/generation/utils.py", line 1622, in generate
    result = self._sample(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/generation/utils.py", line 2791, in _sample
    outputs = self(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py", line 1359, in forward
    outputs = self.model(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py", line 1227, in forward
    layer_outputs = decoder_layer(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py", line 945, in forward
    hidden_states, router_logits = self.block_sparse_moe(hidden_states)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py", line 869, in forward
    idx, top_x = torch.where(expert_mask[expert_idx])
KeyboardInterrupt
