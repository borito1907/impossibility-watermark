/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[2024-05-22 12:13:34,041][__main__][INFO] - Starting to watermark...
[2024-05-22 12:13:34,041][__main__][INFO] - Prompt: Write a 250 word essay on the role of power and its impact on characters in the Lord of the Rings 
        series. How does the ring symbolize power, and what does Tolkien suggest about the nature of power?

        Answer:
[2024-05-22 12:13:34,041][__main__][INFO] - Getting the watermarker...
[2024-05-22 12:13:34,042][watermarker][INFO] - Using device: cuda
[2024-05-22 12:13:34,042][root][INFO] - Device: auto
[2024-05-22 12:13:34,042][model_builders.pipeline][INFO] - Initializing MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ
INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.
INFO - The layer lm_head is not quantized.
[2024-05-22 12:13:37,434][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2024-05-22 12:13:46,300][auto_gptq.utils.accelerate_utils][WARNING] - Some weights of the model checkpoint at /local1/borito1907/.cache/models--MaziyarPanahi--Meta-Llama-3-70B-Instruct-GPTQ/snapshots/46c7afccd4f9345a3d43c1468fde1034cf0a0932/model.safetensors were not used when initializing LlamaForCausalLM: set(). This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
[2024-05-22 12:13:47,992][semstamp][INFO] - {'max_new_tokens': 1024, 'do_sample': True, 'temperature': 0.7, 'top_p': 0.95, 'top_k': 40, 'repetition_penalty': 1.1, 'bad_words_ids': [[198]], 'min_new_tokens': 195}
[2024-05-22 12:13:47,992][semstamp][INFO] - Initializing embedder model.
[2024-05-22 12:13:47,992][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v1
[2024-05-22 12:13:48,868][sentence_transformers.SentenceTransformer][INFO] - Use pytorch device_name: cuda
[2024-05-22 12:13:48,935][semstamp][INFO] - Finished initializing embedder model.
initializing random projection LSH model
loading SBERT base model...
[2024-05-22 12:13:48,940][__main__][INFO] - Got the watermarker. Generating watermarked text...
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.87it/s]
[2024-05-22 12:13:49,312][model_builders.pipeline][INFO] - Prompt: tensor([[ 8144,   264,   220,  5154,  3492,  9071,   389,   279,  3560,   315,
          2410,   323,  1202,  5536,   389,  5885,   304,   279, 10425,   315,
           279, 47380,   720,   286,  4101,    13]], device='cuda:0')
/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.
  warn_deprecated(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/watermarker_tester.py", line 29, in test
    watermarked_text = watermarker.generate(prompt)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/watermarker.py", line 50, in generate
    completion = self.generate_watermarked_outputs(prompt)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/semstamp.py", line 75, in generate_watermarked_outputs
    return self._lsh_generate_watermarked_outputs(prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/semstamp.py", line 85, in _lsh_generate_watermarked_outputs
    response = lsh_reject_completion(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/SemStamp/sampling_lsh_utils.py", line 112, in lsh_reject_completion
    new_text_ids[0, text_ids.size(1):], skip_special_tokens=True)
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
TypeError: string indices must be integers, not 'tuple'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
