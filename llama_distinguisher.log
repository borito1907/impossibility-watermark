[2024-05-02 17:19:43,295][root][INFO] - Configuration: attack_args:
  prompt: null
  prompt_file: ./inputs/dynamic_prompts.csv
  prompt_num: 6
  watermarked_text: null
  watermarked_text_path: null
  watermarked_text_num: 1
  num_steps: 100
  patience: 100
  stop_at_removal: false
  num_successful_steps: 25
  results_dir: null
  model_cache_dir: ./.cache
  save_name: null
  use_watermark: true
  backtrack_patience: 25
  is_completion: false
  json_path: null
  json_index: 0
  is_continuation: false
  prev_csv_file: null
  cuda: '1'
generator_args:
  model_name_or_path: MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ
  revision: main
  model_cache_dir: ${attack_args.model_cache_dir}
  device_map: auto
  trust_remote_code: true
  max_new_tokens: 1024
  do_sample: true
  temperature: 0.7
  top_p: 0.95
  top_k: 40
  repetition_penalty: 1.1
  watermark_score_threshold: 5.0
  cuda: ${attack_args.cuda}
oracle_args:
  num_retries: 5
  use_system_profile: true
  system_profile: You are an impartial judge tasked to evaluate the quality of two
    prompt responses.
  model_name_or_path: MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ
  revision: main
  model_cache_dir: ${attack_args.model_cache_dir}
  device_map: auto
  trust_remote_code: true
  max_new_tokens: 1024
  do_sample: true
  temperature: 0.7
  top_p: 0.95
  top_k: 40
  repetition_penalty: 1.1
  cuda: ${attack_args.cuda}
  is_completion: ${attack_args.is_completion}
  watermark_score_threshold: 5.0
mutator_args:
  use_old_mutator: false
  use_system_profile: true
  system_profile: You are a copy editor tasked to enforce text quality.
  num_retries: 5
  model_name_or_path: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
  revision: main
  model_cache_dir: ${attack_args.model_cache_dir}
  device_map: ${generator_args.device_map}
  trust_remote_code: ${generator_args.trust_remote_code}
  max_new_tokens: ${generator_args.max_new_tokens}
  do_sample: ${generator_args.do_sample}
  temperature: ${generator_args.temperature}
  top_p: ${generator_args.top_p}
  top_k: ${generator_args.top_k}
  repetition_penalty: ${generator_args.repetition_penalty}
  cuda: ${attack_args.cuda}
  use_pydantic_parser: false
watermark_args:
  name: umd
  gamma: 0.25
  delta: 2.0
  seeding_scheme: selfhash
  ignore_repeated_ngrams: true
  normalizers: []
  z_threshold: 0.5
  device: cuda
distinguisher:
  entropy: 1
  output_1: 1
  attack_id_1: 5
  output_2: 3
  attack_id_2: 1
  log_suffix: ''
  num_trials: 10
  num_repetitions: 5
  mutation_num: -1
  matcher: local

[2024-05-02 17:19:43,432][distinguish][INFO] - Initializing a new Distinguisher pipeline from cfg...
[2024-05-02 17:19:43,432][model_builders.pipeline][INFO] - Initializing MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ
[2024-05-02 17:19:43,591][datasets][INFO] - PyTorch version 2.2.1 available.
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/optimized_distinguisher.py", line 114, in <module>
    main()
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
          ^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/optimized_distinguisher.py", line 95, in main
    distinguisher_object = Distinguisher(cfg.generator_args)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/distinguish.py", line 56, in __init__
    self.pipeline = PipeLineBuilder(self.cfg)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/impossibility-watermark/model_builders/pipeline.py", line 36, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3751, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4185, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/transformers/modeling_utils.py", line 792, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.12/site-packages/accelerate/utils/modeling.py", line 387, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
KeyboardInterrupt
