/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
CUDA extension not installed.
CUDA extension not installed.
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[2024-06-20 15:42:54,186][__main__][INFO] - Starting to watermark...
[2024-06-20 15:42:54,189][__main__][INFO] - Prompt: List the steps to process a return for an online purchase.
[2024-06-20 15:42:54,189][__main__][INFO] - Getting the watermarker...
[2024-06-20 15:42:54,189][watermarker][INFO] - Using device: cuda
[2024-06-20 15:42:54,189][model_builders.pipeline][INFO] - Initializing MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.
[2024-06-20 15:42:54,303][auto_gptq.modeling._base][WARNING] - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.
WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:
1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.
2. You are using pytorch without CUDA support.
3. CUDA and nvcc are not installed in your device.
[2024-06-20 15:42:54,303][auto_gptq.modeling._base][WARNING] - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:
1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.
2. You are using pytorch without CUDA support.
3. CUDA and nvcc are not installed in your device.
INFO - The layer lm_head is not quantized.
[2024-06-20 15:43:12,082][auto_gptq.modeling._base][INFO] - The layer lm_head is not quantized.
[2024-06-20 15:44:00,999][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/watermarked_text_generator.py", line 47, in <module>
    test()
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/local1/borito1907/impossibility-watermark/watermarked_text_generator.py", line 24, in test
    watermarker = get_watermarker(cfg, only_detect=False)
  File "/local1/borito1907/impossibility-watermark/watermarker_factory.py", line 13, in get_watermarker
    return SemStampWatermarker(cfg, **kwargs)
  File "/local1/borito1907/impossibility-watermark/watermarkers/semstamp.py", line 40, in __init__
    super().__init__(cfg, pipeline, n_attempts, **kwargs)
  File "/local1/borito1907/impossibility-watermark/watermarker.py", line 22, in __init__
    self.pipeline = PipeLineBuilder(self.cfg.generator_args)
  File "/local1/borito1907/impossibility-watermark/model_builders/pipeline.py", line 28, in __init__
    self._init_model(self.cfg)
  File "/local1/borito1907/impossibility-watermark/model_builders/pipeline.py", line 53, in _init_model
    self.model = AutoGPTQForCausalLM.from_quantized(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/auto_gptq/modeling/auto.py", line 135, in from_quantized
    return quant_func(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/auto_gptq/modeling/_base.py", line 1246, in from_quantized
    accelerate.utils.modeling.load_checkpoint_in_model(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 1673, in load_checkpoint_in_model
    loaded_checkpoint = load_state_dict(checkpoint_file, device_map=device_map)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 1435, in load_state_dict
    return safe_load_file(checkpoint_file, device=list(device_map.values())[0])
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/safetensors/torch.py", line 313, in load_file
    result[k] = f.get_tensor(k)
KeyboardInterrupt
