/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to /home/borito1907/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[2024-07-30 11:45:05,795][__main__][INFO] - Starting to watermark...
[2024-07-30 11:45:05,803][__main__][INFO] - Prompt: In what contexts would you say “Вот черт!”?
[2024-07-30 11:45:05,803][__main__][INFO] - Prompt ID: 2267393233
[2024-07-30 11:45:05,803][__main__][INFO] - Getting the watermarker...
[2024-07-30 11:45:05,803][watermarker][INFO] - Using device: cuda:0
[2024-07-30 11:45:05,803][model_builders.pipeline][INFO] - Initializing facebook/opt-6.7b
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [01:38<01:38, 98.67s/it]Downloading shards: 100%|██████████| 2/2 [02:11<00:00, 60.22s/it]Downloading shards: 100%|██████████| 2/2 [02:11<00:00, 65.99s/it]
[2024-07-30 11:47:19,240][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.
  warn_deprecated(
[2024-07-30 11:47:29,332][watermarkers.semstamp][INFO] - Setting up generating components...
[2024-07-30 11:47:29,527][watermarkers.semstamp][INFO] - {'max_new_tokens': 1024, 'do_sample': True, 'temperature': 1, 'top_p': 0.95, 'top_k': 40, 'repetition_penalty': 1.1, 'bad_words_ids': [[50118]], 'min_new_tokens': 245}
[2024-07-30 11:47:29,527][watermarkers.semstamp][INFO] - Initializing embedder model.
[2024-07-30 11:47:29,527][watermarkers.semstamp][INFO] - Using the generic SentenceTransformer...
[2024-07-30 11:47:29,527][sentence_transformers.SentenceTransformer][INFO] - Use pytorch device_name: cuda
[2024-07-30 11:47:29,527][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v1
[2024-07-30 11:47:30,572][watermarkers.semstamp][INFO] - Finished initializing embedder model.
initializing random projection LSH model
loading SBERT base model...
[2024-07-30 11:47:30,573][__main__][INFO] - {'prompt': None, 'prompt_file': './data/WQE/dev.csv', 'prompt_num': 126, 'is_completion': False, 'generation_stats_file_path': './inputs/dev_semstamp_last_3/gen_stats/prompt_126.csv', 'watermarked_text_file_name': None, 'generator_args': {'model_name_or_path': 'facebook/opt-6.7b', 'revision': 'main', 'model_cache_dir': './.cache/', 'device_map': 'auto', 'trust_remote_code': True, 'max_new_tokens': 1024, 'min_new_tokens': 128, 'do_sample': True, 'temperature': 1, 'top_p': 0.95, 'top_k': 40, 'repetition_penalty': 1.1, 'watermark_score_threshold': 5.0, 'diversity_penalty': 0}, 'watermark_args': {'name': 'semstamp_lsh', 'gamma': 0.25, 'delta': 0.01, 'seeding_scheme': 'selfhash', 'ignore_repeated_ngrams': True, 'normalizers': [], 'z_threshold': 0.5, 'device': 'cuda', 'only_detect': False, 'embedder': '', 'sp_mode': 'lsh', 'sp_dim': 3, 'lmbd': 0.25, 'max_new_tokens': 255, 'min_new_tokens': 245, 'max_trials': 50, 'critical_max_trials': 75, 'cc_path': '', 'train_data': '', 'len_prompt': 32, 'use_fine_tuned': False}, 'attack_args': {'prompt': None, 'prompt_file': './inputs/dynamic_prompts.csv', 'prompt_num': 6, 'watermarked_text': None, 'watermarked_text_path': None, 'watermarked_text_num': 1, 'num_steps': 2000, 'patience': 250, 'stop_at_removal': False, 'num_successful_steps': 100, 'results_dir': None, 'model_cache_dir': '/local1/borito1907/.cache/', 'save_name': None, 'use_watermark': False, 'backtrack_patience': 50, 'is_completion': False, 'json_path': None, 'json_index': 0, 'is_continuation': False, 'prev_csv_file': None, 'cuda': '1'}, 'watermarked_text_file_path': './inputs/dev_semstamp_last_3/watermarked_texts.csv'}
[2024-07-30 11:47:30,573][__main__][INFO] - Got the watermarker. Generating watermarked text...
[2024-07-30 11:47:30,573][watermarkers.semstamp][INFO] - Passing the following prompt to the LSH reject completion function:
 In what contexts would you say “Вот черт!”?
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]
/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
Error executing job with overrides: ['++watermark_args.name=semstamp_lsh', '++watermark_args.embedder=', '++watermark_args.delta=0.01', '++watermark_args.sp_mode=lsh', '++watermark_args.sp_dim=3', '++watermark_args.lmbd=0.25', '++watermark_args.max_new_tokens=255', '++watermark_args.min_new_tokens=245', '++watermark_args.max_trials=50', '++watermark_args.critical_max_trials=75', '++watermark_args.cc_path=', '++watermark_args.train_data=', '++watermark_args.len_prompt=32', '++watermark_args.z_threshold=0.5', '++watermark_args.use_fine_tuned=True', '++prompt_file=./data/WQE/dev.csv', '++prompt_num=126', '++is_completion=False', '++generator_args.temperature=1', '++generator_args.diversity_penalty=0', '++generation_stats_file_path=./inputs/dev_semstamp_last_3/gen_stats/prompt_126.csv', '++watermark_args.use_fine_tuned=False', '++watermarked_text_file_path=./inputs/dev_semstamp_last_3/watermarked_texts.csv']
Traceback (most recent call last):
  File "/local1/borito1907/impossibility-watermark/watermarked_text_generator.py", line 38, in test
    watermarked_text = watermarker.generate(prompt)
  File "/local1/borito1907/impossibility-watermark/watermarker.py", line 51, in generate
    completion = self.generate_watermarked_outputs(prompt)
  File "/local1/borito1907/impossibility-watermark/watermarkers/semstamp.py", line 129, in generate_watermarked_outputs
    return self._lsh_generate_watermarked_outputs(prompt)
  File "/local1/borito1907/impossibility-watermark/watermarkers/semstamp.py", line 301, in _lsh_generate_watermarked_outputs
    response = self._lsh_reject_completion(prompt, stats_csv_path = self.cfg.generation_stats_file_path)
  File "/local1/borito1907/impossibility-watermark/watermarkers/semstamp.py", line 180, in _lsh_reject_completion
    candidate_text, candidate_text_ids = self.generate_sentence(text, text_ids, stopping_criteria)
  File "/local1/borito1907/impossibility-watermark/watermarkers/semstamp.py", line 102, in generate_sentence
    candidate_text, candidate_text_ids = gen_sent(model = self.model,
  File "/local1/borito1907/impossibility-watermark/watermarkers/SemStamp/sampling_utils.py", line 111, in gen_sent
    outputs = model.generate(
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/generation/utils.py", line 1839, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/local1/borito1907/anaconda3/envs/watermark/lib/python3.10/site-packages/transformers/generation/utils.py", line 1267, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 23, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
